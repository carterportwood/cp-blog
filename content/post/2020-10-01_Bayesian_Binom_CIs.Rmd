---
title: Getting Better Binomial Confidence Intervals
author: "Carter Portwood"
date: '2020-10-01'
tags:
  - R
  - confidence intervals
  - binomial
  - Bayesian
  - binom
output: html_document
---

```{r global-options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

In a prior post, I showed how to compare expected interval lengths and coverages. In this post, I'll show how you can leverage your prior knowledge to get better binomial confidence intervals.

When constructing a binomial confidence interval, it's rare that one has *no* prior information about what the proportion of interest is. For instance, even if I know nothing about the proportion of defects at a factory, for most factories, I can safely assume that it's more likely to be 10% than it is to be 90%. We can use that information to make our intervals more efficient.

Suppose $\pi(p)$, our prior for the distribution of $p$, is $Beta(1, 10)$:

```{r}
library(tidyverse)
library(binom)
library(knitr)

p = seq(0.005, 1-0.005, 0.005)
methods = c('bayes', 'wilson')

prior.shape1 = 1 # this is your prior belief
prior.shape2 = 10

plot(p, dbeta(p, prior.shape1, prior.shape2), type='l', 
     main='Prior Distribution for p', ylab='density')
```

If  $p$ is truly from this distribution, then using a Bayesian interval with this prior of course offers large improvements in interval length compared to the Wilson interval. To calculate the expected length and coverage probability, you simply take the expectation (integral) over the true distribution of $p$:
$$E_p C(p,n) =  \int_0^1 C(p,n) f(p)\  dp$$

where $f(p) \sim Beta(1, 10)$ is the pdf of $p$ and $C(p,n)$ is the coverage probability. For the Bayesian interval, $C(p,n)$ also depends on one's choice of prior $\pi(p)$, which also happens to be $Beta(1, 10)$ in this case. Let's compare to the Wilson interval method.

```{r}
n = c(8, 15, 50)

shape1 = prior.shape1 # this is the true distribution
shape2 = prior.shape2


coverage = binom.coverage(p, n, 
                            method = methods, 
                            type='central', 
                            prior.shape1 = prior.shape1,
                            prior.shape2 = prior.shape2)
len = binom.length(p, n, 
                    method = methods, 
                    type='central', 
                    prior.shape1 = prior.shape1,
                    prior.shape2 = prior.shape2)

coverage = coverage %>%
  left_join(len, by=c('method', 'n', 'p')) %>%
  mutate(
    p.density = dbeta(p, shape1, shape2)
  )

coverage %>%
  group_by(method, n) %>%
  summarise(
    'Expected Length' = sum(p.density*length)/sum(p.density),
    'Expected Coverage' = sum(p.density*coverage)/sum(p.density)
  ) %>%
  kable(digits=2)
```

Of course, this is lucky case that your prior and the true distribution are the exact same. If you were always that good, you wouldn't need to collect any data! It's not a fair comparison to assume that $f_p = \pi$. So let's make our prior a little less confident, $Beta(1, 3)$, shown below, and still have $p \sim Beta(1, 10)$. 

```{r}
prior.shape1 = 1 # this is your prior belief
prior.shape2 = 3

shape1 = 1 # this is the true distribution
shape2 = 10

plot(p, dbeta(p, prior.shape1, prior.shape2), type='l', 
     main='Prior Distribution for p', ylab='density')
```


```{r echo=FALSE}
coverage = binom.coverage(p, n, 
                            method = methods, 
                            type='central', 
                            prior.shape1 = prior.shape1,
                            prior.shape2 = prior.shape2)
len = binom.length(p, n, 
                    method = methods, 
                    type='central', 
                    prior.shape1 = prior.shape1,
                    prior.shape2 = prior.shape2)

coverage = coverage %>%
  left_join(len, by=c('method', 'n', 'p')) %>%
  mutate(
    p.density = dbeta(p, shape1, shape2)
  )

coverage %>%
  group_by(method, n) %>%
  summarise(
    'Expected Length' = sum(p.density*length)/sum(p.density),
    'Expected Coverage' = sum(p.density*coverage)/sum(p.density)
  ) %>% 
  kable(digits=2)
```

So even when our prior is somewhat conservative, using a Bayesian interval can improve expected interval length. But what if you get your prior wrong? Suppose our prior is $Beta(3, 1)$, shown below, while $p$ still follows a $Beta(1, 10)$ distribution.

```{r echo=FALSE}
prior.shape1 = 3 # this is your prior belief
prior.shape2 = 1

shape1 = 1 # this is the true distribution
shape2 = 10

plot(p, dbeta(p, prior.shape1, prior.shape2), type='l', 
     main='Prior Distribution for p', ylab='density')
```

In this case, where your prior is just off-base, the Bayesian interval has inadequate expected coverage and even higher expected length.

```{r echo=FALSE}
coverage = binom.coverage(p, n, 
                            method = methods, 
                            type='central', 
                            prior.shape1 = prior.shape1,
                            prior.shape2 = prior.shape2)
len = binom.length(p, n, 
                    method = methods, 
                    type='central', 
                    prior.shape1 = prior.shape1,
                    prior.shape2 = prior.shape2)

coverage = coverage %>%
  left_join(len, by=c('method', 'n', 'p')) %>%
  mutate(
    p.density = dbeta(p, shape1, shape2)
  )

coverage %>%
  group_by(method, n) %>%
  summarise(
    'Expected Length' = sum(p.density*length)/sum(p.density),
    'Expected Coverage' = sum(p.density*coverage)/sum(p.density)
  ) %>% 
  kable(digits=2)
```


So, we've seen how using an informed prior for a Bayesian interval can improve interval efficiency, especially with limited sample size. But a mis-specified prior will lead to bigger intervals, poor coverage, and incorrect inference. 

Of course, this is all based on the idea that it's appropriate to treat $p$ as from some distribution. But you can still utilize this to pick whether its worth using a Bayesian CI, even if you philosophically believe $p$ is fixed. Without knowing $p$, it makes sense to try to choose the best method for the "region" in which you believe $p$ may reside. In addition, I've seen comparisons of the "average coverage" and "average width" of confidence interval methods. It's worth noting that this is equivalent to assuming $p \sim Unif(0, 1)$. Oftentimes, that's even more nonsensical than assuming $p$ is from some other distribution.

If you have an idea for what the 'distribution' of $p$, then you can use the above code to calculate the expected coverage and length for various interval methods. When in doubt, the `binom` package makes exploring different methods very easy.
