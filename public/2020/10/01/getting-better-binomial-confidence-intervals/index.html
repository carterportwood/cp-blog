<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.74.3" />


<title>Getting Better Binomial Confidence Intervals - Carter Portwood</title>
<meta property="og:title" content="Getting Better Binomial Confidence Intervals - Carter Portwood">


  <link href='/images/avocado.jpg' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/avocado.jpg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/post/">Blog</a></li>
    
    <li><a href="https://github.com/carterportwood">GitHub</a></li>
    
    <li><a href="https://www.linkedin.com/in/carter-portwood/">LinkedIn</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">4 min read</span>
    

    <h1 class="article-title">Getting Better Binomial Confidence Intervals</h1>

    
    <span class="article-date">2020-10-01</span>
    

    <div class="article-content">
      


<p>In a prior post, I showed how to compare expected interval lengths and coverages. In this post, I’ll show how you can leverage your prior knowledge to get better binomial confidence intervals.</p>
<p>When constructing a binomial confidence interval, it’s rare that one has <em>no</em> prior information about what the proportion of interest is. For instance, even if I know nothing about the proportion of defects at a factory, for most factories, I can safely assume that it’s more likely to be 10% than it is to be 90%. We can use that information to make our intervals more efficient.</p>
<p>Suppose <span class="math inline">\(\pi(p)\)</span>, our prior for the distribution of <span class="math inline">\(p\)</span>, is <span class="math inline">\(Beta(1, 10)\)</span>:</p>
<pre class="r"><code>library(tidyverse)
library(binom)
library(knitr)

p = seq(0.005, 1-0.005, 0.005)
methods = c(&#39;bayes&#39;, &#39;wilson&#39;)

prior.shape1 = 1 # this is your prior belief
prior.shape2 = 10

plot(p, dbeta(p, prior.shape1, prior.shape2), type=&#39;l&#39;, 
     main=&#39;Prior Distribution for p&#39;, ylab=&#39;density&#39;)</code></pre>
<p><img src="/post/2020-10-01_Bayesian_Binom_CIs_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>If <span class="math inline">\(p\)</span> is truly from this distribution, then using a Bayesian interval with this prior of course offers large improvements in interval length compared to the Wilson interval. To calculate the expected length and coverage probability, you simply take the expectation (integral) over the true distribution of <span class="math inline">\(p\)</span>:
<span class="math display">\[E_p C(p,n) =  \int_0^1 C(p,n) f(p)\  dp\]</span></p>
<p>where <span class="math inline">\(f(p) \sim Beta(1, 10)\)</span> is the pdf of <span class="math inline">\(p\)</span> and <span class="math inline">\(C(p,n)\)</span> is the coverage probability. For the Bayesian interval, <span class="math inline">\(C(p,n)\)</span> also depends on one’s choice of prior <span class="math inline">\(\pi(p)\)</span>, which also happens to be <span class="math inline">\(Beta(1, 10)\)</span> in this case. Let’s compare to the Wilson interval method.</p>
<pre class="r"><code>n = c(8, 15, 50)

shape1 = prior.shape1 # this is the true distribution
shape2 = prior.shape2


coverage = binom.coverage(p, n, 
                            method = methods, 
                            type=&#39;central&#39;, 
                            prior.shape1 = prior.shape1,
                            prior.shape2 = prior.shape2)
len = binom.length(p, n, 
                    method = methods, 
                    type=&#39;central&#39;, 
                    prior.shape1 = prior.shape1,
                    prior.shape2 = prior.shape2)

coverage = coverage %&gt;%
  left_join(len, by=c(&#39;method&#39;, &#39;n&#39;, &#39;p&#39;)) %&gt;%
  mutate(
    p.density = dbeta(p, shape1, shape2)
  )

coverage %&gt;%
  group_by(method, n) %&gt;%
  summarise(
    &#39;Expected Length&#39; = sum(p.density*length)/sum(p.density),
    &#39;Expected Coverage&#39; = sum(p.density*coverage)/sum(p.density)
  ) %&gt;%
  kable(digits=2)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">method</th>
<th align="right">n</th>
<th align="right">Expected Length</th>
<th align="right">Expected Coverage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">bayes</td>
<td align="right">8</td>
<td align="right">0.21</td>
<td align="right">0.95</td>
</tr>
<tr class="even">
<td align="left">bayes</td>
<td align="right">15</td>
<td align="right">0.19</td>
<td align="right">0.95</td>
</tr>
<tr class="odd">
<td align="left">bayes</td>
<td align="right">50</td>
<td align="right">0.13</td>
<td align="right">0.95</td>
</tr>
<tr class="even">
<td align="left">wilson</td>
<td align="right">8</td>
<td align="right">0.40</td>
<td align="right">0.95</td>
</tr>
<tr class="odd">
<td align="left">wilson</td>
<td align="right">15</td>
<td align="right">0.29</td>
<td align="right">0.95</td>
</tr>
<tr class="even">
<td align="left">wilson</td>
<td align="right">50</td>
<td align="right">0.15</td>
<td align="right">0.95</td>
</tr>
</tbody>
</table>
<p>Of course, this is lucky case that your prior and the true distribution are the exact same. If you were always that good, you wouldn’t need to collect any data! It’s not a fair comparison to assume that <span class="math inline">\(f_p = \pi\)</span>. So let’s make our prior a little less confident, <span class="math inline">\(Beta(1, 3)\)</span>, shown below, and still have <span class="math inline">\(p \sim Beta(1, 10)\)</span>.</p>
<pre class="r"><code>prior.shape1 = 1 # this is your prior belief
prior.shape2 = 3

shape1 = 1 # this is the true distribution
shape2 = 10

plot(p, dbeta(p, prior.shape1, prior.shape2), type=&#39;l&#39;, 
     main=&#39;Prior Distribution for p&#39;, ylab=&#39;density&#39;)</code></pre>
<p><img src="/post/2020-10-01_Bayesian_Binom_CIs_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<table>
<thead>
<tr class="header">
<th align="left">method</th>
<th align="right">n</th>
<th align="right">Expected Length</th>
<th align="right">Expected Coverage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">bayes</td>
<td align="right">8</td>
<td align="right">0.32</td>
<td align="right">0.96</td>
</tr>
<tr class="even">
<td align="left">bayes</td>
<td align="right">15</td>
<td align="right">0.25</td>
<td align="right">0.95</td>
</tr>
<tr class="odd">
<td align="left">bayes</td>
<td align="right">50</td>
<td align="right">0.14</td>
<td align="right">0.95</td>
</tr>
<tr class="even">
<td align="left">wilson</td>
<td align="right">8</td>
<td align="right">0.40</td>
<td align="right">0.95</td>
</tr>
<tr class="odd">
<td align="left">wilson</td>
<td align="right">15</td>
<td align="right">0.29</td>
<td align="right">0.95</td>
</tr>
<tr class="even">
<td align="left">wilson</td>
<td align="right">50</td>
<td align="right">0.15</td>
<td align="right">0.95</td>
</tr>
</tbody>
</table>
<p>So even when our prior is somewhat conservative, using a Bayesian interval can improve expected interval length. But what if you get your prior wrong? Suppose our prior is <span class="math inline">\(Beta(3, 1)\)</span>, shown below, while <span class="math inline">\(p\)</span> still follows a <span class="math inline">\(Beta(1, 10)\)</span> distribution.</p>
<p><img src="/post/2020-10-01_Bayesian_Binom_CIs_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>In this case, where your prior is just off-base, the Bayesian interval has inadequate expected coverage and even higher expected length.</p>
<table>
<thead>
<tr class="header">
<th align="left">method</th>
<th align="right">n</th>
<th align="right">Expected Length</th>
<th align="right">Expected Coverage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">bayes</td>
<td align="right">8</td>
<td align="right">0.49</td>
<td align="right">0.72</td>
</tr>
<tr class="even">
<td align="left">bayes</td>
<td align="right">15</td>
<td align="right">0.35</td>
<td align="right">0.72</td>
</tr>
<tr class="odd">
<td align="left">bayes</td>
<td align="right">50</td>
<td align="right">0.17</td>
<td align="right">0.78</td>
</tr>
<tr class="even">
<td align="left">wilson</td>
<td align="right">8</td>
<td align="right">0.40</td>
<td align="right">0.95</td>
</tr>
<tr class="odd">
<td align="left">wilson</td>
<td align="right">15</td>
<td align="right">0.29</td>
<td align="right">0.95</td>
</tr>
<tr class="even">
<td align="left">wilson</td>
<td align="right">50</td>
<td align="right">0.15</td>
<td align="right">0.95</td>
</tr>
</tbody>
</table>
<p>So, we’ve seen how using an informed prior for a Bayesian interval can improve interval efficiency, especially with limited sample size. But a mis-specified prior will lead to bigger intervals, poor coverage, and incorrect inference.</p>
<p>Of course, this is all based on the idea that it’s appropriate to treat <span class="math inline">\(p\)</span> as from some distribution. But you can still utilize this to pick whether its worth using a Bayesian CI, even if you philosophically believe <span class="math inline">\(p\)</span> is fixed. Without knowing <span class="math inline">\(p\)</span>, it makes sense to try to choose the best method for the “region” in which you believe <span class="math inline">\(p\)</span> may reside. In addition, I’ve seen comparisons of the “average coverage” and “average width” of confidence interval methods. It’s worth noting that this is equivalent to assuming <span class="math inline">\(p \sim Unif(0, 1)\)</span>. Oftentimes, that’s even more nonsensical than assuming <span class="math inline">\(p\)</span> is from some other distribution.</p>
<p>If you have an idea for what the ‘distribution’ of <span class="math inline">\(p\)</span>, then you can use the above code to calculate the expected coverage and length for various interval methods. When in doubt, the <code>binom</code> package makes exploring different methods very easy.</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

