<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Carter Portwood</title>
    <link>/</link>
    <description>Recent content on Carter Portwood</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Jun 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Two Types of Methodological Complexity</title>
      <link>/2021/06/25/two-types-of-methodological-complexity/</link>
      <pubDate>Fri, 25 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/06/25/two-types-of-methodological-complexity/</guid>
      <description>I often see statements like, “As the complexity of the model increases, the variance will increase and bias will decrease.” This type of statement is referring to a specific type of model complexity, namely, the model’s capacity to overfit data. Below, I call this the “degrees of freedom complexity,” for lack of a better term.1
Maybe a bit pedantically, I’ll point out that there’s another important type of complexity that is distinct from the degrees of freedom complexity, what I’ll call “interpretation complexity”.</description>
    </item>
    
    <item>
      <title>Getting Better Binomial Confidence Intervals</title>
      <link>/2020/10/01/getting-better-binomial-confidence-intervals/</link>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/01/getting-better-binomial-confidence-intervals/</guid>
      <description>In a prior post, I showed how to compare expected interval lengths and coverages. In this post, I’ll show how you can leverage your prior knowledge to get better binomial confidence intervals.
When constructing a binomial confidence interval, it’s rare that one has no prior information about what the proportion of interest is. For instance, even if I know nothing about the proportion of defects at a factory, for most factories, I can safely assume that it’s more likely to be 10% than it is to be 90%.</description>
    </item>
    
    <item>
      <title>Visualizing Inspection Sampling Scheme Selection with &#39;gganimate&#39;</title>
      <link>/2020/09/06/visualizing-inspection-sampling-scheme-selection-with-gganimate/</link>
      <pubDate>Sun, 06 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/06/visualizing-inspection-sampling-scheme-selection-with-gganimate/</guid>
      <description>It’s common in the world of quality inspection to assess the proportion of defects produced by a process. To accept or reject a lot, acceptance sampling is often used. For a binary outcome like defect/not defect, this can take the form of randomly sampling \(n\) items and counting the number of defects, \(x\). The lot or process passes if \(x \le c\) and is rejected otherwise, where \(c\) is the maximum number of allowable defects.</description>
    </item>
    
    <item>
      <title>Comparing Binomial Confidence Interval Methods</title>
      <link>/2020/09/04/comparing-binomial-confidence-interval-methods/</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/04/comparing-binomial-confidence-interval-methods/</guid>
      <description>Binomial Confidence Intervals Are Not Simple Sometimes it can be easy to take confidence intervals for granted. They’re one of the first things you learn in statistics, and you see them everywhere. We’re so used to \(\hat{\theta} \pm 1.96 \ \hat{se}(\hat{\theta})\) that it can be surprising that something as fundamental as binomial confidence intervals is still a subject of debate and confusion. In this post, I’ll discuss some ways you can compare confidence interval methods for binomial proportions with the easy-to-use binom package in R.</description>
    </item>
    
  </channel>
</rss>